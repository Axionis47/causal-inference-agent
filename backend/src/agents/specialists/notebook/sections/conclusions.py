"""Conclusions section renderer."""

from datetime import datetime

import numpy as np
from nbformat.v4 import new_markdown_cell

from src.agents.base import AnalysisState

from ..helpers import deduplicate_effects, generate_llm_narrative


async def render_conclusions(state: AnalysisState, *, llm=None, system_prompt: str = "") -> list:
    """Generate conclusions section with LLM narrative."""
    cells = []

    effects = deduplicate_effects(state.treatment_effects)
    avg_effect = np.mean([e.estimate for e in effects]) if effects else 0
    all_positive = all(e.estimate > 0 for e in effects) if effects else False
    all_negative = all(e.estimate < 0 for e in effects) if effects else False

    # Try LLM-generated conclusion
    llm_context = {
        "treatment": state.treatment_variable,
        "outcome": state.outcome_variable,
        "n_methods": len(effects),
        "avg_effect": f"{avg_effect:.4f}",
        "direction": "positive" if all_positive else "negative" if all_negative else "mixed",
    }

    if state.sensitivity_results:
        llm_context["sensitivity"] = "; ".join(
            f"{s.method}: {s.robustness_value:.2f}" for s in state.sensitivity_results
        )

    if state.critique_history:
        latest = state.critique_history[-1]
        llm_context["critique_decision"] = latest.decision.value
        if latest.issues:
            llm_context["key_issues"] = "; ".join(latest.issues[:3])

    if state.domain_knowledge and state.domain_knowledge.get("uncertainties"):
        uncerts = state.domain_knowledge["uncertainties"]
        if uncerts:
            first = uncerts[0]
            if isinstance(first, dict):
                llm_context["uncertainty"] = first.get("issue", str(first))
            else:
                llm_context["uncertainty"] = str(first)

    llm_conclusion = ""
    if llm:
        llm_conclusion = await generate_llm_narrative(llm, system_prompt, "conclusions", llm_context)

    # Build conclusions section
    conclusion_md = "## Conclusions\n\n"

    if llm_conclusion:
        conclusion_md += llm_conclusion + "\n\n"
    else:
        # Fallback: template conclusion
        conclusion_md += "### Key Findings\n\n"
        conclusion_md += f"The analysis estimated the effect of **{state.treatment_variable}** "
        conclusion_md += f"on **{state.outcome_variable}** using {len(effects)} method(s).\n\n"
        conclusion_md += f"- **Average Treatment Effect**: {avg_effect:.4f}\n"
        direction = "positive" if all_positive else "negative" if all_negative else "mixed"
        conclusion_md += f"- **Direction consistency**: All methods agree on a **{direction}** effect.\n\n"

    # Recommendations (from orchestrator)
    if state.recommendations:
        conclusion_md += "### Recommendations\n\n"
        for i, rec in enumerate(state.recommendations, 1):
            conclusion_md += f"{i}. {rec}\n"
        conclusion_md += "\n"

    # Standard limitations
    conclusion_md += "### Limitations\n\n"
    conclusion_md += "- **Observational data**: Cannot rule out unmeasured confounding\n"
    conclusion_md += "- **Model assumptions**: Each method relies on specific assumptions\n"
    conclusion_md += "- **External validity**: Results may not generalize to other populations\n\n"

    cells.append(new_markdown_cell(conclusion_md))

    # Reproducibility footer
    repro_md = f"""---

### Reproducibility Information

- **Analysis Date**: {datetime.utcnow().strftime('%Y-%m-%d')}
- **Job ID**: {state.job_id}
- **Dataset**: {state.dataset_info.name or state.dataset_info.url}
- **Treatment Variable**: {state.treatment_variable}
- **Outcome Variable**: {state.outcome_variable}

This notebook was automatically generated by the Causal Inference Orchestrator.
"""
    cells.append(new_markdown_cell(repro_md))

    return cells
